{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset and initializing vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import chromadb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, fbeta_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading friends data. this dataset is already cleaned from unessesary colunms and nan values\n",
    "data = pd.read_csv('clean_friends.csv')\n",
    "\n",
    "# \"only use 1 procent of the data fast debugging\n",
    "# data = data.sample(frac=0.01, random_state=42)\n",
    "\n",
    "# ensure all data['text'] are strings\n",
    "data['text'] = data['text'].astype(str)\n",
    "\n",
    "# split the data into two parts: one for the vector database and one for testing\n",
    "data_vector_db, data_rag_test = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# initialize Chroma client and create a collection\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"friends_collection\")\n",
    "\n",
    "# prepare the metadata/labels\n",
    "labels = []\n",
    "for label in data_vector_db['label']:\n",
    "    labels.append({'label': label})\n",
    "\n",
    "# add text documents to the collection with metadata and ids\n",
    "collection.add(\n",
    "    documents=data_vector_db['text'].tolist(), # e.g. ['jag blir mobbad', ... , 'klassen är stökig.']\n",
    "    metadatas=labels, # e.g. [{'label': 0.0}, {'label': 1.0}, ... , {'label': 1.0}]\n",
    "    ids=data_vector_db.index.astype(str).tolist() #  e.g.  ['3614', '2325', '1032', '2323']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting the LLM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# to store predictions and actual labels\n",
    "predictions = []\n",
    "actual_labels = []\n",
    "\n",
    "# query each text from data_rag_test and classify\n",
    "for count, (index, row) in enumerate(data_rag_test.iterrows(), 1):\n",
    "    print('\\n' * 10 + '-' * 300)\n",
    "    print(f\" Processing {count}/{len(data_rag_test)}...\")  # indicates progress\n",
    "   \n",
    "    text_to_classify = row['text']\n",
    "    actual_labels.append(row['label'])\n",
    "   \n",
    "    # querying the collection for the most similar 100 texts\n",
    "    most_similar_collection = collection.query(\n",
    "        query_texts=[text_to_classify],\n",
    "        n_results=100\n",
    "\n",
    "    )\n",
    "\n",
    "    # extract texts, labels, and distances\n",
    "    texts = most_similar_collection['documents'][0] \n",
    "    labels = most_similar_collection['metadatas'][0]\n",
    "    \n",
    "    # organize results by label, collect the first 3 closest matches for each label\n",
    "    results_by_label = {1: [], 0: []}\n",
    "    for i in range(len(texts)):\n",
    "        label = labels[i]['label']\n",
    "        if len(results_by_label[label]) < 3:\n",
    "            results_by_label[label].append(texts[i])\n",
    "            if len(results_by_label[0]) == 3 and len(results_by_label[1]) == 3:\n",
    "                break  # stop once we have 3 texts for each label\n",
    "\n",
    "    # prepare a string of example texts for the prompt\n",
    "    similar_texts_string = \"\"\n",
    "    for label in results_by_label:\n",
    "        for text in results_by_label[label]:\n",
    "            similar_texts_string += f\"- {text}, label: {label}\\n\"\n",
    "\n",
    "    # make RAG prediction\n",
    "    valid_response = False\n",
    "    while not valid_response:\n",
    "\n",
    "        system_prompt = f\"\"\"\\nKlassificera den sista textmeningen utifrån om personen som skrivit den verkar mobbad i skolmiljön. Om det tyder på mobbning, svara endast med '1.0'. Annars, svara endast med '0.0'. Klassificeringen gäller enbart om författaren av texten är mobbad. Innehåll som bara är stötande eller våldsamt innebär inte automatiskt att texten ska märkas som '1.0'.\\n\\nHär är sex märkta exempel:\\n{similar_texts_string}\\n\\nKlassifiera texten i user prompten: \"\"\"\n",
    "        user_prompt = f\"{text_to_classify}\\n\\n(OBS! svara endast '1.0' eller '0.0', inget annat!)\"\n",
    "        print(\"\\nPROMPT TO LLAMA 3:\\n\" + '- ' * 300 + system_prompt + user_prompt + '\\n' + '- ' * 300)\n",
    "            \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(\"\\n - LLama response:  \" + completion.choices[0].message.content)\n",
    "\n",
    "        response = completion.choices[0].message.content.strip()\n",
    "        if response == '1.0' or response == '0.0':\n",
    "            valid_response = True\n",
    "            predicted_label = float(response)\n",
    "            predictions.append(predicted_label)\n",
    "            actual_label = row['label']\n",
    "            print(\" - Actual Label: \", actual_label)  \n",
    "        \n",
    "        print('-' * 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(actual_labels, predictions, target_names=['Not Bullying', '    Bullying'])\n",
    "f3_score_bullying = fbeta_score(actual_labels, predictions, beta=3, pos_label=1)\n",
    "f3_score_non_bullying = fbeta_score(actual_labels, predictions, beta=3, pos_label=0)\n",
    "f3_score_macro = (f3_score_bullying + f3_score_non_bullying) / 2\n",
    "\n",
    "print(report)\n",
    "print(f\"F3 Score for Bullying: {f3_score_bullying}\")\n",
    "print(f\"F3 Score for Non-Bullying: {f3_score_non_bullying}\")\n",
    "print(f\"F3 Score Macro: {f3_score_macro}\")\n",
    "\n",
    "# evaluate the model using a confusion matrix\n",
    "conf_matrix = confusion_matrix(actual_labels, predictions)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# less confusing explanation\n",
    "print(f\"\\n  {tn + fp + fn + tp} cases in total in test set.\")\n",
    "print(f\"\\n  {tp} out of {tp + fn} actual bullying cases identified (true Positives).\")\n",
    "print(f\"  {fp} false positives (non-bullying identified as bullying).\")\n",
    "print(f\"  {fn} bullying cases missed (false negatives).\")\n",
    "print(f\"  {tn} correctly identified non-bullying cases (true negatives).\")\n",
    "\n",
    "\n",
    "# plot the confusion matrix of the best threshold\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title(f'Confusion Matrix, F3-Score ({f3_score_bullying:.2f})')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
