{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with FastText pre-trained word embeddings\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "# loading the dataset\n",
    "data = pd.read_csv('new_preprocessed_friends_data.csv')\n",
    "texts = data['text'].values\n",
    "labels = data['label'].values\n",
    "\n",
    "# text tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and FastText word embeddings\n",
    "downloaded at: https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "\n",
    "# path to potentially existing embedding matrix created in erlier runs\n",
    "file_path = 'embedding_matrix.npy'\n",
    "\n",
    "# load the embedding matrix if it already exists, to save time (takes 10 minutes to create)\n",
    "if os.path.exists(file_path):\n",
    "    # Load the existing embedding matrix\n",
    "    embedding_matrix = np.load(file_path)\n",
    "    print('embedding matrix found')\n",
    "else:\n",
    "    print('embedding matrix NOT found')\n",
    "    model_path = 'cc.sv.300.bin'\n",
    "    fasttext_model = load_facebook_vectors(model_path)\n",
    "\n",
    "    # create an embedding matrix mapping each word from friends dataset to its corresponding vector in the FastText model\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, fasttext_model.vector_size))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = fasttext_model[word]\n",
    "            if embedding_vector is not None:    \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # save the newly created embedding matrix\n",
    "    np.save(file_path, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning, grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# compute the class weights\n",
    "class_weights = compute_class_weight('balanced', classes=[0, 1], y=labels)\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# custom F3 scorer for class label 1\n",
    "def f3_scorer(true_labels, predicted_labels):\n",
    "    return fbeta_score(true_labels, predicted_labels, beta=3, pos_label=1)\n",
    "\n",
    "scorer = make_scorer(f3_scorer)\n",
    "\n",
    "# creation of the CNN model with it's layers\n",
    "def create_model(optimizer, kernel_regularizer_val, dropout_rate):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                  output_dim=300,\n",
    "                  weights=[embedding_matrix],\n",
    "                  input_length=max_length,\n",
    "                  trainable=False),\n",
    "        Conv1D(filters=256, kernel_size=3, activation='relu', \n",
    "               kernel_regularizer=l2(kernel_regularizer_val)),\n",
    "        Dropout(dropout_rate),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(units=64, activation='relu', kernel_regularizer=l2(kernel_regularizer_val)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# wrap the model for scikit-learn compatibility\n",
    "model = KerasClassifier(build_fn=create_model, epochs=30, batch_size=32)\n",
    "\n",
    "# Parameter grid for tuning\n",
    "param_grid = {\n",
    "    'optimizer': ['rmsprop', 'adam'],\n",
    "    'kernel_regularizer_val': [0.01, 0.02],\n",
    "    'dropout_rate': [0.5, 0.6]\n",
    "}\n",
    "\n",
    "\n",
    "# stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# gridSearchCV setup with the F3 scorer(bullying class) and class weights\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scorer, cv=skf, verbose=1)\n",
    "\n",
    "# run the gridsearch\n",
    "grid_search.fit(X, labels, class_weight=class_weights)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# print the best parameters and best score\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best F3 score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, fbeta_score, confusion_matrix\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# perform Stratified 5-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_y_true = []\n",
    "all_y_pred_prob = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, labels)):\n",
    "    print(f\"\\nFold {fold+1}\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "    \n",
    "    model = create_model(optimizer='adam', kernel_regularizer_val=0.01, dropout_rate=0.6)\n",
    "    \n",
    "    early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "    \n",
    "    # create model using the best parameters found\n",
    "    model = create_model(optimizer=best_params['optimizer'], \n",
    "                         kernel_regularizer_val=best_params['kernel_regularizer_val'], \n",
    "                         dropout_rate=best_params['dropout_rate'])\n",
    "    \n",
    "    # train the model\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=32, \n",
    "                        validation_data=(X_test, y_test), \n",
    "                        verbose=2, callbacks=[early_stopping_monitor], \n",
    "                        class_weight=class_weights)\n",
    "\n",
    "    # extract loss values for clearer access\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    # plot training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.title(f'Fold {fold+1}: Training and Validation Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()  \n",
    "\n",
    "    # predict probabilities\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred_prob_positive = y_pred_prob[:, 0]\n",
    "    \n",
    "    all_y_true.extend(y_test)\n",
    "    all_y_pred_prob.extend(y_pred_prob_positive)\n",
    "\n",
    "# convert lists to numpy arrays for easier handling\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred_prob = np.array(all_y_pred_prob)\n",
    "\n",
    "# plot histograms for the combined probabilities\n",
    "y_true_0 = all_y_true == 0\n",
    "y_true_1 = all_y_true == 1\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(all_y_pred_prob[y_true_0], bins=80, alpha=0.5, label='Non-bullying (label 0)', color='blue')\n",
    "plt.hist(all_y_pred_prob[y_true_1], bins=80, alpha=0.5, label='Bullying (label 1)', color='red')\n",
    "plt.title('Combined Distribution of Predicted Probabilities')\n",
    "plt.xlabel('Probability of Positive Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal threshold that maximizes f3-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = None\n",
    "best_conf_matrix = None\n",
    "best_f3_score = -1\n",
    "\n",
    "# evaluate different thresholds on combined results\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "for threshold in thresholds:\n",
    "    y_pred_classes = (all_y_pred_prob >= threshold).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_y_true, y_pred_classes, labels=[0, 1], average=None, zero_division=0)\n",
    "    f3_0 = fbeta_score(all_y_true, y_pred_classes, beta=3, average='binary', pos_label=0, zero_division=0)\n",
    "    f3_1 = fbeta_score(all_y_true, y_pred_classes, beta=3, average='binary', pos_label=1, zero_division=0)\n",
    "    f3_overall = (f3_0 + f3_1) / 2\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_y_true, y_pred_classes)\n",
    "\n",
    "    print(f\"\\nThreshold {threshold}:\")\n",
    "    print(\"Average report:\")\n",
    "    print(f\"Bullying class: Precision {precision[1]:.2f}, Recall {recall[1]:.2f}, F1-score {f1[1]:.2f}, F3-score {f3_1:.3f}\")\n",
    "    print(f\"Non-bullying class: Precision {precision[0]:.2f}, Recall {recall[0]:.2f}, F1-score {f1[0]:.2f}, F3-score {f3_0:.3f}\")\n",
    "    print(f\"Overall F3-score: {f3_overall:.3f}\")\n",
    "    \n",
    "    # check if this threshold has the best F3 score\n",
    "    if f3_1 > best_f3_score:\n",
    "        best_f3_score = f3_1\n",
    "        best_threshold = threshold\n",
    "        best_conf_matrix = conf_matrix\n",
    "\n",
    "# plot confusion matrix\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(best_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title(f'Confusion Matrix for Threshold {best_threshold}: with Highest F3-Score ({best_f3_score:.3f})')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
